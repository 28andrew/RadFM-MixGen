{
 "cells": [
  {
   "cell_type": "code",
   "id": "fc3ad1d1-7f54-4c22-ae6c-b5308680ab66",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key='INSERT API KEY HERE',\n",
    "\n",
    ")\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": text,\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=1024,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "136cc215-f88f-4adc-842e-ac5d39cdef5c",
   "metadata": {},
   "source": [
    "completion.choices[0].message.content.replace('\\n', '')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ae93e5cf-d5d0-4ca2-bcc7-5598365c9edf",
   "metadata": {},
   "source": [
    "import pickle\n",
    "\n",
    "with open('data_with_imgs.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0c328801-7fc5-4d3e-adb5-0c48eed95470",
   "metadata": {},
   "source": [
    "print(data[9])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2ccaf610-298c-41e1-bc48-8d68f44517e5",
   "metadata": {},
   "source": [
    "import openai\n",
    "import random\n",
    "import time\n",
    "\n",
    "def retry_with_exponential_backoff(\n",
    "    func,\n",
    "    initial_delay: float = 1,\n",
    "    exponential_base: float = 2,\n",
    "    jitter: bool = True,\n",
    "    max_retries: int = 10,\n",
    "    errors: tuple = (openai.RateLimitError,),\n",
    "):\n",
    "    \"\"\"Retry a function with exponential backoff.\"\"\"\n",
    "\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Initialize variables\n",
    "        num_retries = 0\n",
    "        delay = initial_delay\n",
    "\n",
    "        # Loop until a successful response or max_retries is hit or an exception is raised\n",
    "        while True:\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "\n",
    "            # Retry on specified errors\n",
    "            except errors as e:\n",
    "                # Increment retries\n",
    "                num_retries += 1\n",
    "\n",
    "                # Check if max retries has been reached\n",
    "                if num_retries > max_retries:\n",
    "                    raise Exception(\n",
    "                        f\"Maximum number of retries ({max_retries}) exceeded.\"\n",
    "                    )\n",
    "\n",
    "                # Increment the delay\n",
    "                delay *= exponential_base * (1 + jitter * random.random())\n",
    "\n",
    "                # Sleep for the delay\n",
    "                time.sleep(delay)\n",
    "\n",
    "            # Raise exceptions for any errors not specified\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "\n",
    "    return wrapper"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c87d2a28-70bc-4f56-a32f-7fd458149e10",
   "metadata": {},
   "source": [
    "@retry_with_exponential_backoff\n",
    "def perform_openai_call(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text,\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=1024,\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "    return completion.choices[0].message.content.replace('\\n', '')\n",
    "\n",
    "def create_prompt(a, b, typ):\n",
    "    prompt = f'I am running an experiment when I am making data augmentation on pairs of radiology images and corresponding findings. I am merging the images and I need you to merge the text ({typ} 1 and {typ} 2). The merged text will describe this merged image and needs to include all the information from both texts but include the most specific fact if there is ambiguity. For example, if one text says no abnormalities, but another describes abnormalities, then describe the abnormalities in the merged text.\\n'\n",
    "    prompt += f'{typ} 1: {a}\\n'\n",
    "    prompt += f'{typ} 2: {b}\\n'\n",
    "    prompt += f'DO THIS TASK CAREFULLY WITHOUT FORGETTING ANY DETAILS. KEEP THE LENGTH SIMILAR TO THE LONGEST {typ}s. NOW BELOW THIS TEXT, OUTPUT THE MERGED FINDINGS IN THE SAME FORMAT (MATCHING CASING IF NEEDED) WITHOUT ADDING NEW LINES, WITHOUT A PREFIX LIKE \\'MERGED {typ}s:\\', AND WITHOUT OTHER SPECIAL FORMATTING:\\n'\n",
    "    return prompt\n",
    "\n",
    "def process_item(d):\n",
    "    # Impression\n",
    "    if d['impression_a'] and d['impression_b']:\n",
    "        d['impression'] = perform_openai_call(create_prompt(d['impression_a'], d['impression_b'], 'impression'))\n",
    "    else:\n",
    "        d['impression'] = d['impression_a'] or d['impression_b']\n",
    "    # Finding\n",
    "    if d['findings_a'] and d['findings_b']:\n",
    "        d['findings'] = perform_openai_call(create_prompt(d['findings_a'], d['findings_b'], 'finding'))\n",
    "    else:\n",
    "        d['findings'] = d['findings_a'] or d['findings_b']\n",
    "    return d"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a181cf86-cd17-46d2-b6c0-9dbd39c42972",
   "metadata": {},
   "source": [
    "prompts = []\n",
    "\n",
    "def process_item_fake(d):\n",
    "    # Impression\n",
    "    if d['impression_a'] and d['impression_b']:\n",
    "        d['impression'] = prompts.append(create_prompt(d['impression_a'], d['impression_b'], 'impression'))\n",
    "    else:\n",
    "        d['impression'] = d['impression_a'] or d['impression_b']\n",
    "    # Finding\n",
    "    if d['findings_a'] and d['findings_b']:\n",
    "        d['findings'] = prompts.append(create_prompt(d['findings_a'], d['findings_b'], 'finding'))\n",
    "    else:\n",
    "        d['findings'] = d['findings_a'] or d['findings_b']\n",
    "    return d"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1c451c4d-0c8d-414b-be42-ceef172387df",
   "metadata": {},
   "source": [
    "from tqdm.notebook import tqdm\n",
    "for item in tqdm(data):\n",
    "    process_item_fake(item)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ca0fecab-d550-4982-9fc6-358fd42a5c3a",
   "metadata": {},
   "source": [
    "prompts[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "15405fc3-d919-438a-b8af-910766ec6148",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "batch_jsons = []\n",
    "for idx, prompt in enumerate(prompts):\n",
    "    #{\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-3.5-turbo-0125\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},{\"role\": \"user\", \"content\": \"Hello world!\"}],\"max_tokens\": 1000}}\n",
    "    batch_jsons.append(json.dumps({\n",
    "        'custom_id': str(idx),\n",
    "        'method': 'POST',\n",
    "        'url': '/v1/chat/completions',\n",
    "        'body': {\n",
    "            'model': 'gpt-3.5-turbo',\n",
    "            'messages': [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            'max_tokens': 1024\n",
    "        }\n",
    "    }))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3b11d203-4e84-46b4-97b8-03294cd61dc2",
   "metadata": {},
   "source": [
    "batch_jsons[1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1b3a83ee-f965-4f11-bef4-08ef18f128a5",
   "metadata": {},
   "source": [
    "batch_size = 10000\n",
    "total_lines = len(batch_jsons)\n",
    "number_of_files = (total_lines + batch_size - 1) // batch_size\n",
    "\n",
    "for i in range(number_of_files):\n",
    "    start_index = i * batch_size\n",
    "    end_index = min((i + 1) * batch_size, total_lines)\n",
    "    file_name = f'openai_batch{i+1}.jsonl'\n",
    "    with open(file_name, 'w') as file:\n",
    "        file.writelines([line + '\\n' for line in batch_jsons[start_index:end_index]])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "42635195-4c21-4ca7-b92b-4030151009de",
   "metadata": {},
   "source": [
    "len(prompts), len(batch_jsons), len(batch_jsons[:50000]) + len(batch_jsons[50000:])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f5cc721-296e-48a2-9351-f461be0b78d6",
   "metadata": {},
   "source": [
    "batch_file_ids = []\n",
    "for i in range(1, number_of_files + 1):\n",
    "    batch_input_file = client.files.create(\n",
    "      file=open(f\"openai_batch{i}.jsonl\", \"rb\"),\n",
    "      purpose=\"batch\"\n",
    "    )\n",
    "    batch_file_ids.append(batch_input_file.id)\n",
    "batch_file_ids"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# !!!!\n",
    "# GO TO OPENAI DASHBOARD TO SUBMIT THESE BATCH REQUESTS. They will process within 24 hours. Then save the responses in files batch1.jsonl to batch10.jsonl.\n",
    "# !!!!"
   ],
   "id": "d6b27cee9aa16c79",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e8279532-8a79-4b82-ae56-c181395356e9",
   "metadata": {},
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "responses = {}\n",
    "\n",
    "for i in tqdm(range(1, 10+1)):\n",
    "    with open(f'batch{i}.jsonl', 'r') as file:\n",
    "        for line in file:\n",
    "            d = json.loads(line)\n",
    "            responses[int(d['custom_id'])] = d['response']['body']['choices'][0]['message']['content']\n",
    "\n",
    "responses[0], len(responses)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ff52ff55-c863-4432-9786-abbe28c394b6",
   "metadata": {},
   "source": [
    "call_idx = 0\n",
    "\n",
    "def process_item_offline(d):\n",
    "    global call_idx\n",
    "    # Impression\n",
    "    if d['impression_a'] and d['impression_b']:\n",
    "        d['impression'] = responses[call_idx]\n",
    "        call_idx += 1\n",
    "    else:\n",
    "        d['impression'] = d['impression_a'] or d['impression_b']\n",
    "    # Finding\n",
    "    if d['findings_a'] and d['findings_b']:\n",
    "        d['findings'] = responses[call_idx]\n",
    "        call_idx += 1\n",
    "    else:\n",
    "        d['findings'] = d['findings_a'] or d['findings_b']\n",
    "    return d"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "44eb72c7-8381-4816-9be2-2cdf30471369",
   "metadata": {},
   "source": [
    "for item in tqdm(data):\n",
    "    process_item_offline(item)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "01c4411a-08bb-4ab0-8252-eba56377fc1a",
   "metadata": {},
   "source": [
    "len(data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ba36957d-3dbc-4f3b-b4ea-253215612d34",
   "metadata": {},
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MixgenDataset(Dataset):\n",
    "    def __init__(self, input_data):\n",
    "        self.data = input_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d2bb4d89-c9f3-4a86-9ce7-3f7e77435359",
   "metadata": {},
   "source": [
    "import pickle\n",
    "partition_size = len(data) // 4\n",
    "\n",
    "data1 = data[:partition_size]\n",
    "data2 = data[partition_size:2*partition_size]\n",
    "data3 = data[2*partition_size:3*partition_size]\n",
    "data4 = data[3*partition_size:]\n",
    "\n",
    "with open('mixgen1.pkl', 'wb') as file:\n",
    "    pickle.dump(data1, file)\n",
    "\n",
    "with open('mixgen2.pkl', 'wb') as file:\n",
    "    pickle.dump(data2, file)\n",
    "\n",
    "with open('mixgen3.pkl', 'wb') as file:\n",
    "    pickle.dump(data3, file)\n",
    "\n",
    "with open('mixgen4.pkl', 'wb') as file:\n",
    "    pickle.dump(data4, file)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "142ebdd8-f0ae-4537-97ab-ae0e08adfd45",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
